{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_TAGGING.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "17OnrebNU1NBlBc2HSTc9wW5DUojX9-es",
      "authorship_tag": "ABX9TyOXzZ5pN9lFMZvGyPGpIMjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NguenDuyNhat/DuyNhat_CS2018_UIT/blob/master/POS_TAGGING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YiHSGdayir0"
      },
      "source": [
        "# **Truy cập vào nơi chứa data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABNfEkdUDBpd"
      },
      "source": [
        "cd /content/drive/My Drive/CoQUY/Data_coQuy/Kho ngu lieu 10000 cau duoc gan nhan tu loai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LyWmDd2yugg"
      },
      "source": [
        "# **Khai báo các thư viện cần thiết**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hUuq1XZDOaa"
      },
      "source": [
        "import os \r\n",
        "import pandas as pd\r\n",
        "import joblib\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDoTcfuy9Jx"
      },
      "source": [
        "# **Load dữ liệu thô**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1_DyKcDpDN"
      },
      "source": [
        "raw_data=[]\r\n",
        "for i in os.listdir(\"/content/drive/My Drive/CoQUY/Data_coQuy/Kho ngu lieu 10000 cau duoc gan nhan tu loai\"):  \r\n",
        "        with open(i) as f:\r\n",
        "          raw_data.append(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3xWheHAEV_w"
      },
      "source": [
        "raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8btb4_UzFco"
      },
      "source": [
        "# **Tách dữ liệu thô thành từng câu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6kxnxdhOcl1"
      },
      "source": [
        "data_X=[]\r\n",
        "for i in range(len(raw_data)):\r\n",
        "  b=raw_data[i].split('\\n')\r\n",
        "  data_X.append(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhO7EzWKzm0"
      },
      "source": [
        "X=[]\r\n",
        "for i in data_X:\r\n",
        "    for j in i:\r\n",
        "        X.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mISnwFPUPAFF"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxUx6Igzf7H"
      },
      "source": [
        "# **Có tất cả 10521 câu được gán sẵn nhãn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oll82ueP7rH"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50KRP-W5zsmX"
      },
      "source": [
        "# **Phân dữ liệu thành 3 tập: train,dev,test.**\r\n",
        "# **Tập train chiếm 75% dữ liệu: 7365 câu**\r\n",
        "# **Tập val chiếm 15% dữ liệu : 1578 câu** \r\n",
        "# **Tập test chiếm 10% dữ liệu: 1578 câu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc3TJrT4PgBY"
      },
      "source": [
        "X_test, X_train  = train_test_split(X, test_size=0.7,random_state=101)\r\n",
        "X_test, X_val  = train_test_split(X_test, test_size=0.5,random_state=101) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoK8zos3QEd9"
      },
      "source": [
        "print(len(X_train),len(X_test),len(X_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofzJnqO4NE7U"
      },
      "source": [
        "# **Xử lí các câu trong tập train thành dạng dataframe: 2 cột**\r\n",
        "# **Cột thứ 1: là từ được tách ra từ câu**\r\n",
        "# **Cột thứ 2: là nhãn của từ được gán**\r\n",
        "# **Sau khi đưa tập train về dataframe, tiến hành lưu thành file train.csv: gồm 2 cột và 149946 dòng**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob3yQkvMSBmF"
      },
      "source": [
        "data_train=[]\r\n",
        "for senten in X_train:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_train.append(wp)\r\n",
        "words_train=[]\r\n",
        "pos_tag_train=[]\r\n",
        "for i in data_train:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_train.append(index[0]) # ( ) : . , ; ? ! + - ... < > { }\r\n",
        "            pos_tag_train.append(index[-1])\r\n",
        "df_train=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_train[\"words\"]=words_train\r\n",
        "df_train[\"pos_tag\"]=pos_tag_train\r\n",
        "df_train.to_csv('/content/drive/MyDrive/CoQUY/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJHcm67SN5OQ"
      },
      "source": [
        "# **Sau khi đưa tập validation về dataframe, tiến hành lưu thành file val.csv: gồm 2 cột và 33167 dòng**\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flHYUrhlVTU1"
      },
      "source": [
        "data_val=[]\r\n",
        "for senten in X_val:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_val.append(wp)\r\n",
        "words_val=[]\r\n",
        "pos_tag_val=[]\r\n",
        "for i in data_val:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_val.append(index[0]) # ( ) : . , ; ? ! + - ... < > { }\r\n",
        "            pos_tag_val.append(index[-1]) # ( ) : . , ; ? ! +\r\n",
        "df_val=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_val[\"words\"]=words_val\r\n",
        "df_val[\"pos_tag\"]=pos_tag_val\r\n",
        "df_val.to_csv('/content/drive/MyDrive/CoQUY/val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWfMF9IiXFv7"
      },
      "source": [
        "# **Sau khi đưa tập test về dataframe, tiến hành lưu thành file test.csv: gồm 2 cột và 32578 dòng**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eE6RuqJW38R"
      },
      "source": [
        "data_test=[]\r\n",
        "for senten in X_test:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_test.append(wp)\r\n",
        "words_test=[]\r\n",
        "pos_tag_test=[]\r\n",
        "for i in data_test:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_test.append(index[0])\r\n",
        "            pos_tag_test.append(index[-1])\r\n",
        "df_test=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_test[\"words\"]=words_test\r\n",
        "df_test[\"pos_tag\"]=pos_tag_test\r\n",
        "df_test.to_csv('/content/drive/MyDrive/CoQUY/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrjnNyVkrVEV"
      },
      "source": [
        "# **Gọi Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0watvnAiaMYX"
      },
      "source": [
        "word_emb = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/CoQUY/baomoi.model.bin\", binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgKbp8ZUtuNz"
      },
      "source": [
        "# **Đưa các từ ở df_train[words] về dạng thích hợp với mô hình để đưa vào huấn luyện=> X_features_train**\r\n",
        "# **Đưa các nhãn => X_labels_train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFznYCs2a6iM"
      },
      "source": [
        "X_features_train=[]\r\n",
        "Y_labels_train=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_train=set (pos_tag_train)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_train):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "#df_train[\"pos_tag\"]=df_train[\"pos_tag\"].apply(lambda x: convert(x))\r\n",
        "\r\n",
        "for i in pos_tag_train:\r\n",
        "    Y_labels_train.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_train[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_train.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_train.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rTQWwMVunBG"
      },
      "source": [
        "# **Đưa các từ ở df_val[words] về dạng thích hợp với mô hình để đưa vào huấn luyện => X_features_val**\r\n",
        "# **Đưa các nhãn => X_labels_val**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEABD0mawsKs"
      },
      "source": [
        "X_features_val=[]\r\n",
        "Y_labels_val=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_val=set (pos_tag_val)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_val):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "#df_val[\"pos_tag\"]=df_val[\"pos_tag\"].apply(lambda x: convert(x))\r\n",
        "\r\n",
        "for i in pos_tag_val :\r\n",
        "    Y_labels_val.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_val[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_val.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_val.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoPFjHCvxgu"
      },
      "source": [
        "# **Đưa các từ ở df_test[words] về dạng thích hợp với mô hình để đưa vào huấn luyện => X_features_test**\r\n",
        "# **Đưa các nhãn => X_labels_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-C1Q82yoycC"
      },
      "source": [
        "X_features_test=[]\r\n",
        "Y_labels_test=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_test=set (pos_tag_test)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_test):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "\r\n",
        "for i in pos_tag_test :\r\n",
        "    Y_labels_test.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_test[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_test.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_test.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpnywzluv4gZ"
      },
      "source": [
        "# **Xem lại kích thước ở mỗi features và label của mỗi tập**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tayHXpDO1Hff"
      },
      "source": [
        "print(len(X_features_train),len(Y_labels_train))\r\n",
        "print(len(X_features_val),len(Y_labels_val))\r\n",
        "print(len(X_features_test),len(Y_labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rw1T6RWwJw9"
      },
      "source": [
        "# **Sử dụng mô hình SVM để train cho tập train gồm 149946 features và labels, sau đó dự đoán trên tập val**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrnmgwkxaNp"
      },
      "source": [
        "model1=SVC().fit(X_features_train,Y_labels_train)\r\n",
        "predict=model1.predict(X_features_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDKtzmX4wY7p"
      },
      "source": [
        "# **Kết quả và confusion matrix khi mô hình train xong và dự đoán trên tập val**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-03Kjqxx_S"
      },
      "source": [
        "print('Accuracy: ',accuracy_score(Y_labels_val, predict))\r\n",
        "print('Classification report: \\n',classification_report(Y_labels_val, predict))\r\n",
        "print('Confusion matrix: \\n',confusion_matrix(Y_labels_val, predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBPGaIpkxFoJ"
      },
      "source": [
        "# **Lưu mô hình để khi demo không cần phải train lại**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0LQJv33yEKG"
      },
      "source": [
        "filename = '/content/drive/MyDrive/CoQUY/model_SVM_result.sav' \r\n",
        "joblib.dump(model1, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvgukgoxOe3"
      },
      "source": [
        "# **Load mô hình ra để sử dụng cho nhanh trong quá trình demo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FIUZib67UiG"
      },
      "source": [
        "model_loaded_result=joblib.load(\"/content/drive/MyDrive/CoQUY/model_SVM_result.sav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMD1uq95xZkN"
      },
      "source": [
        "# **Dùng mô hình đã train để dự đoán tập test và in ra kết quả**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWZC0EALod4Q"
      },
      "source": [
        "result_test=model_loaded_result.predict(X_features_test)\r\n",
        "print('Accuracy: ',accuracy_score(Y_labels_test, result_test))\r\n",
        "print('Classification report: \\n',classification_report(Y_labels_test, result_test))\r\n",
        "print('Confusion matrix: \\n',confusion_matrix(Y_labels_test, result_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpVNLOh8xoNT"
      },
      "source": [
        "# **Demo: sử dụng công cụ tách từ từ pyvi, chạy đoạn code dưới để install pyvi sau đó chạy đoạn code cuối để nhập câu cần gán nhãn từ loại và sẽ được kết quả**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzjeqXGZw3mB"
      },
      "source": [
        "! pip install pyvi\r\n",
        "from pyvi import ViTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ang4vkRaw4aA"
      },
      "source": [
        "sentence=input(str())\r\n",
        "word_emb = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/CoQUY/baomoi.model.bin\", binary=True)\r\n",
        "def raw_senten(sentence):\r\n",
        "    list_senten=[]\r\n",
        "    np_senten=[]\r\n",
        "    list_senten=ViTokenizer.tokenize(sentence).split()\r\n",
        "    return list_senten\r\n",
        "def raw_word(list_senten):\r\n",
        "    np_=[]\r\n",
        "    for i in list_senten:\r\n",
        "        if (i in word_emb.vocab.keys()):\r\n",
        "            np_.append((word_emb.word_vec(i)).reshape(1,400))\r\n",
        "        else:\r\n",
        "            np_.append((np.zeros(400,dtype=float)).reshape(1,400))\r\n",
        "    return np_\r\n",
        "list_senten=raw_senten(sentence)\r\n",
        "np_=raw_word(list_senten)\r\n",
        "a=[]\r\n",
        "for i in np_:\r\n",
        "    a.append(model_loaded_result.predict(i))\r\n",
        "#print(a)\r\n",
        "for i in range (len(list_senten)):\r\n",
        "    print((list_senten[i])+\"/\"+a[i][0],end=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}