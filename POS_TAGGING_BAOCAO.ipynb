{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "POS_TAGGING_BAOCAO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "17OnrebNU1NBlBc2HSTc9wW5DUojX9-es",
      "authorship_tag": "ABX9TyN2cvZ1jn89fyC4bD0aNEcU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABNfEkdUDBpd",
        "outputId": "abc56f34-f4f2-454f-e29d-becb8616afc0"
      },
      "source": [
        "cd /content/drive/My Drive/CoQUY/Data_coQuy/Kho ngu lieu 10000 cau duoc gan nhan tu loai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CoQUY/Data_coQuy/Kho ngu lieu 10000 cau duoc gan nhan tu loai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hUuq1XZDOaa"
      },
      "source": [
        "import os \r\n",
        "import pandas as pd\r\n",
        "import joblib\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1_DyKcDpDN"
      },
      "source": [
        "raw_data=[]\r\n",
        "for i in os.listdir(\"/content/drive/My Drive/CoQUY/Data_coQuy/Kho ngu lieu 10000 cau duoc gan nhan tu loai\"):  \r\n",
        "        with open(i) as f:\r\n",
        "          raw_data.append(f.read())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6kxnxdhOcl1"
      },
      "source": [
        "data_X=[]\r\n",
        "for i in range(len(raw_data)):\r\n",
        "  b=raw_data[i].split('\\n')\r\n",
        "  data_X.append(b)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhO7EzWKzm0"
      },
      "source": [
        "X=[]\r\n",
        "for i in data_X:\r\n",
        "    for j in i:\r\n",
        "        X.append(j)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBUWCxprQqqz"
      },
      "source": [
        "# 10 521 sentences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oll82ueP7rH",
        "outputId": "699f3872-ae99-426c-9545-64012c2167a3"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc3TJrT4PgBY"
      },
      "source": [
        "X_test, X_train  = train_test_split(X, test_size=0.7,random_state=101)\r\n",
        "X_test, X_val  = train_test_split(X_test, test_size=0.5,random_state=101)\r\n",
        "print(len(X_train),len(X_test),len(X_val))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMeYmsf3Q1fT"
      },
      "source": [
        "# Save train : train.csv ( 70%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob3yQkvMSBmF"
      },
      "source": [
        "data_train=[]\r\n",
        "for senten in X_train:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_train.append(wp)\r\n",
        "words_train=[]\r\n",
        "pos_tag_train=[]\r\n",
        "for i in data_train:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_train.append(index[0]) \r\n",
        "            pos_tag_train.append(index[-1])\r\n",
        "df_train=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_train[\"words\"]=words_train\r\n",
        "df_train[\"pos_tag\"]=pos_tag_train\r\n",
        "df_train.to_csv('/content/drive/MyDrive/CoQUY/train.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQdI6Fz6RHK7"
      },
      "source": [
        "# Save validation:  val.csv (15%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flHYUrhlVTU1"
      },
      "source": [
        "data_val=[]\r\n",
        "for senten in X_val:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_val.append(wp)\r\n",
        "words_val=[]\r\n",
        "pos_tag_val=[]\r\n",
        "for i in data_val:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_val.append(index[0]) # ( ) : . , ; ? ! + - ... < > { }\r\n",
        "            pos_tag_val.append(index[-1]) # ( ) : . , ; ? ! +\r\n",
        "df_val=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_val[\"words\"]=words_val\r\n",
        "df_val[\"pos_tag\"]=pos_tag_val\r\n",
        "df_val.to_csv('/content/drive/MyDrive/CoQUY/val.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp9g5Gu0RUSO"
      },
      "source": [
        "# Save test : test.csv (  15%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eE6RuqJW38R"
      },
      "source": [
        "data_test=[]\r\n",
        "for senten in X_test:\r\n",
        "    senten_split=senten.split()\r\n",
        "    for wp in senten_split:\r\n",
        "        data_test.append(wp)\r\n",
        "words_test=[]\r\n",
        "pos_tag_test=[]\r\n",
        "for i in data_test:\r\n",
        "    if (i.count(\"/\")==1):\r\n",
        "        index=i.split(\"/\")\r\n",
        "        if( index[-1].isalpha() or index[-1]==\"(\" or index[-1]==\")\" or index[-1]==\":\" or index[-1]==\".\" or index[-1]==\",\" or index[-1]== \";\" or index[-1]==\"?\"  or index[-1]==\"!\" or index[-1]==\"+\" or index[-1]==\"-\" or index[-1]==\"...\" or index[-1]==\"<\" or index[-1]==\">\" or index[-1]==\"{\" or index[-1]==\"}\"):\r\n",
        "            words_test.append(index[0])\r\n",
        "            pos_tag_test.append(index[-1])\r\n",
        "df_test=pd.DataFrame(columns=(\"words\",\"pos_tag\"))\r\n",
        "df_test[\"words\"]=words_test\r\n",
        "df_test[\"pos_tag\"]=pos_tag_test\r\n",
        "df_test.to_csv('/content/drive/MyDrive/CoQUY/test.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU_WTIvoSJUw"
      },
      "source": [
        "# Load Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0watvnAiaMYX"
      },
      "source": [
        "word_emb = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/CoQUY/baomoi.model.bin\", binary=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5EVFRUSR_H"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFznYCs2a6iM"
      },
      "source": [
        "X_features_train=[]\r\n",
        "Y_labels_train=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_train=set (pos_tag_train)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_train):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "#df_train[\"pos_tag\"]=df_train[\"pos_tag\"].apply(lambda x: convert(x))\r\n",
        "\r\n",
        "for i in pos_tag_train:\r\n",
        "    Y_labels_train.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_train[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_train.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_train.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEABD0mawsKs"
      },
      "source": [
        "X_features_val=[]\r\n",
        "Y_labels_val=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_val=set (pos_tag_val)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_val):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "#df_val[\"pos_tag\"]=df_val[\"pos_tag\"].apply(lambda x: convert(x))\r\n",
        "\r\n",
        "for i in pos_tag_val :\r\n",
        "    Y_labels_val.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_val[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_val.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_val.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-C1Q82yoycC"
      },
      "source": [
        "X_features_test=[]\r\n",
        "Y_labels_test=[]\r\n",
        "def convert(x):\r\n",
        "    return dictionary[x]\r\n",
        "\r\n",
        "set_pos_test=set (pos_tag_test)\r\n",
        "dictionary = {}\r\n",
        "\r\n",
        "for idx, i in enumerate(set_pos_test):\r\n",
        "    dictionary[i]=idx\r\n",
        "\r\n",
        "\r\n",
        "for i in pos_tag_test :\r\n",
        "    Y_labels_test.append(i)\r\n",
        "\r\n",
        "for idx,w in enumerate(df_test[\"words\"]):\r\n",
        "    if (w in word_emb.vocab.keys()):\r\n",
        "        X_features_test.append(word_emb.word_vec(w))\r\n",
        "    else:\r\n",
        "        X_features_test.append(np.zeros(400,dtype=float))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tayHXpDO1Hff",
        "outputId": "76e35373-37e4-472f-cb37-73ea85652ec9"
      },
      "source": [
        "print(len(X_features_train),len(Y_labels_train))\r\n",
        "print(len(X_features_val),len(Y_labels_val))\r\n",
        "print(len(X_features_test),len(Y_labels_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149946 149946\n",
            "33167 33167\n",
            "32578 32578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrnmgwkxaNp"
      },
      "source": [
        "#model1=SVC().fit(X_features_train,Y_labels_train)\r\n",
        "#predict=model1.predict(X_features_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-03Kjqxx_S",
        "outputId": "bcc71e40-88e9-4f22-95cd-63eaa77d924e"
      },
      "source": [
        "#predict=model1.predict(X_features_val)\r\n",
        "print('Accuracy: ',accuracy_score(Y_labels_val, predict))\r\n",
        "print('Classification report: \\n',classification_report(Y_labels_val, predict))\r\n",
        "print('Confusion matrix: \\n',confusion_matrix(Y_labels_val, predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7358519009859198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           !       0.00      0.00      0.00        82\n",
            "           (       0.00      0.00      0.00       124\n",
            "           )       0.00      0.00      0.00       128\n",
            "           +       0.00      0.00      0.00         1\n",
            "           ,       0.23      1.00      0.38      1832\n",
            "           -       0.00      0.00      0.00       247\n",
            "           .       0.00      0.00      0.00      1241\n",
            "         ...       0.00      0.00      0.00       231\n",
            "           :       1.00      1.00      1.00       241\n",
            "           ;       1.00      1.00      1.00        19\n",
            "           >       1.00      1.00      1.00         8\n",
            "           ?       0.00      0.00      0.00        60\n",
            "           A       0.89      0.78      0.83      2030\n",
            "          Ab       0.00      0.00      0.00         2\n",
            "           B       0.00      0.00      0.00         4\n",
            "           C       0.91      0.80      0.86      1245\n",
            "           E       0.79      0.91      0.84      2110\n",
            "          Eb       0.00      0.00      0.00         2\n",
            "           I       0.67      0.22      0.33        18\n",
            "           L       0.99      0.84      0.91       616\n",
            "           M       0.97      0.49      0.65      1219\n",
            "           N       0.90      0.87      0.89      8150\n",
            "          Nb       0.93      0.44      0.60        61\n",
            "          Nc       0.73      0.45      0.56       826\n",
            "          Np       0.81      0.02      0.04      1408\n",
            "          Nu       0.88      0.63      0.74       157\n",
            "          Ny       0.00      0.00      0.00       213\n",
            "           P       0.92      0.87      0.90      1416\n",
            "           R       0.93      0.77      0.84      2525\n",
            "           S       0.64      0.47      0.55        19\n",
            "           T       0.64      0.21      0.31       209\n",
            "           V       0.90      0.87      0.89      6587\n",
            "          Vb       0.00      0.00      0.00         3\n",
            "           X       0.83      0.47      0.60       115\n",
            "           Y       0.00      0.00      0.00        17\n",
            "           a       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.74     33167\n",
            "   macro avg       0.49      0.39      0.41     33167\n",
            "weighted avg       0.79      0.74      0.73     33167\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ... 54  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0LQJv33yEKG",
        "outputId": "0b275447-e8e3-4dc4-8e7c-0c0f0221898a"
      },
      "source": [
        "#filename = '/content/drive/MyDrive/CoQUY/model_SVM_result.sav' \r\n",
        "#joblib.dump(model1, filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/CoQUY/model_SVM_result.sav']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWZC0EALod4Q",
        "outputId": "45533a38-a8d7-4ad0-80e2-35f4c3999712"
      },
      "source": [
        "#result_test=model_loaded_result.predict(X_features_test)\r\n",
        "print('Accuracy: ',accuracy_score(Y_labels_test, result_test))\r\n",
        "print('Classification report: \\n',classification_report(Y_labels_test, result_test))\r\n",
        "print('Confusion matrix: \\n',confusion_matrix(Y_labels_test, result_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7366320830007981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           !       0.00      0.00      0.00       104\n",
            "           (       0.00      0.00      0.00        94\n",
            "           )       0.00      0.00      0.00        93\n",
            "           ,       0.24      1.00      0.39      1865\n",
            "           -       0.00      0.00      0.00       265\n",
            "           .       0.00      0.00      0.00      1263\n",
            "         ...       0.00      0.00      0.00       208\n",
            "           :       1.00      1.00      1.00       236\n",
            "           ;       1.00      1.00      1.00        12\n",
            "           >       1.00      1.00      1.00         2\n",
            "           ?       0.00      0.00      0.00        65\n",
            "           A       0.88      0.79      0.83      2033\n",
            "          Ab       0.00      0.00      0.00         2\n",
            "           B       0.00      0.00      0.00         4\n",
            "           C       0.91      0.79      0.84      1153\n",
            "           E       0.79      0.92      0.85      2018\n",
            "           I       1.00      0.15      0.26        20\n",
            "           L       0.99      0.81      0.89       594\n",
            "           M       0.96      0.46      0.62      1286\n",
            "           N       0.90      0.86      0.88      7924\n",
            "          Nb       0.73      0.31      0.43        62\n",
            "          Nc       0.78      0.52      0.62       794\n",
            "          Np       0.84      0.03      0.06      1290\n",
            "          Nu       0.87      0.69      0.77       167\n",
            "          Ny       0.00      0.00      0.00       182\n",
            "           P       0.93      0.87      0.90      1463\n",
            "           R       0.91      0.77      0.83      2362\n",
            "           S       0.89      0.65      0.76        26\n",
            "           T       0.80      0.29      0.42       239\n",
            "           V       0.90      0.88      0.89      6598\n",
            "          Vb       1.00      0.20      0.33         5\n",
            "          Vy       0.00      0.00      0.00         1\n",
            "           X       0.84      0.41      0.55       141\n",
            "           Y       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.74     32578\n",
            "   macro avg       0.56      0.42      0.45     32578\n",
            "weighted avg       0.79      0.74      0.73     32578\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0 58  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGGNS4GUSd8U"
      },
      "source": [
        "# Word segmentation: pyvi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzjeqXGZw3mB",
        "outputId": "a6bcee2a-fbf4-40b0-d004-9afeae17eb3f"
      },
      "source": [
        "! pip install pyvi\r\n",
        "from pyvi import ViTokenizer"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5MB 3.3MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.19.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRoAZiW4Ssh4"
      },
      "source": [
        "# **DEMO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yobMwNSei5X"
      },
      "source": [
        "model_loaded_result=joblib.load(\"/content/drive/MyDrive/CoQUY/model_SVM_result.sav\")\r\n",
        "word_emb = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/CoQUY/baomoi.model.bin\", binary=True)\r\n",
        "def raw_senten(sentence):\r\n",
        "    list_senten=[]\r\n",
        "    np_senten=[]\r\n",
        "    list_senten=(ViTokenizer.tokenize(sentence)).split()\r\n",
        "    return list_senten\r\n",
        "\r\n",
        "def raw_word(list_senten):\r\n",
        "    np_=[]\r\n",
        "    for i in list_senten:\r\n",
        "        if (i in word_emb.vocab.keys()):\r\n",
        "            np_.append((word_emb.word_vec(i)).reshape(1,400))\r\n",
        "        else:\r\n",
        "            np_.append((np.zeros(400,dtype=float)).reshape(1,400))\r\n",
        "    return np_\r\n",
        "\r\n",
        "def demo(sentence):\r\n",
        "    list_senten=raw_senten(sentence)\r\n",
        "    np_=raw_word(list_senten)\r\n",
        "    a=[]\r\n",
        "    for i in np_:\r\n",
        "        a.append(model_loaded_result.predict(i))\r\n",
        "    annotated_tool=''\r\n",
        "    for i in range (len(list_senten)):\r\n",
        "        #annotated_tool=''\r\n",
        "        annotated_tool=annotated_tool+(list_senten[i])+\"/\"+a[i][0]+\" \"\r\n",
        "    return annotated_tool\r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZXiZOXDSyPE"
      },
      "source": [
        "# ANNOTATION TOOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAucVCT2PVSS",
        "outputId": "d110e71c-deaa-4b9f-d14b-3d80220a6fe0"
      },
      "source": [
        "sentence=str(input())\r\n",
        "print(demo(sentence))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hôm nay trời mưa rất to.\n",
            "hôm_nay/N trời/N mưa/V rất/R to/A ./, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poKph6JmS9KD"
      },
      "source": [
        "# Demo sentences in file.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNL-gxf8JRLh"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/CoQUY/Train_anotator/50 CÂU TUOITRE/DATA.txt\",'r') as f:\r\n",
        "    data=f.read()   \r\n",
        "data=data.split(\"\\n\")\r\n",
        "data_anno=\"\"    \r\n",
        "for i in data:\r\n",
        "    data_annotated=demo(i)\r\n",
        "    data_anno=data_anno+data_annotated+\"\\n\"\r\n",
        "data_anno=data_anno[:-2]\r\n",
        "with open(\"/content/drive/MyDrive/CoQUY/Train_anotator/data_annotated.txt\",\"w\") as f1:\r\n",
        "    f1.write(data_anno)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}